{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ForwardDiff.jl\n",
    "\n",
    "*Student: Jarrett Revels*\n",
    "\n",
    "*Mentors: Miles Lubin, Theodore Papamarkou*\n",
    "\n",
    "*Sponsored by: Julia Summer of Code (Gordon and Betty Moore Foundation)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is ForwardDiff.jl?\n",
    "\n",
    "ForwardDiff.jl is a package that provides a type-based implementation of forward mode AD in the Julia language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can I do with it?\n",
    "\n",
    "Take derivatives, gradients, Jacobians, Hessians, and \"Tensors\" ($3^{\\text{rd}}$ order derivatives) of native Julia functions (or any callable object, really)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does it look like? \n",
    "\n",
    "\n",
    "For the sake of example, let's use ForwardDiff.jl to take derivatives of a toy function $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$:\n",
    "\n",
    "$$ f(\\vec{x}) = \\sum_{i=1}^n \\sin(x_i) + \\prod_{i=1}^n \\tan(x_i) \\cdot \\sum_{i=1}^n \\sqrt{x_i} $$\n",
    "\n",
    "In the Julia REPL:\n",
    "\n",
    "```julia\n",
    "julia> using ForwardDiff\n",
    "\n",
    "julia> f(x) = sum(sin, x) + prod(tan, x) * sum(sqrt, x);\n",
    "\n",
    "julia> x = rand(5) # small size for example's sake\n",
    "5-element Array{Float64,1}:\n",
    " 0.986403\n",
    " 0.140913\n",
    " 0.294963\n",
    " 0.837125\n",
    " 0.650451\n",
    "\n",
    "julia> g = ForwardDiff.gradient(f); # g = ∇f\n",
    "\n",
    "julia> g(x)\n",
    "5-element Array{Float64,1}:\n",
    " 1.01358\n",
    " 2.50014\n",
    " 1.72574\n",
    " 1.10139\n",
    " 1.2445\n",
    "\n",
    "julia> j = ForwardDiff.jacobian(g); # j = J(∇f)\n",
    "\n",
    "julia> j(x)\n",
    "5x5 Array{Float64,2}:\n",
    " 0.585111  3.48083  1.7706    0.994057  1.03257\n",
    " 3.48083   1.06079  5.79299   3.25245   3.37871\n",
    " 1.7706    5.79299  0.423981  1.65416   1.71818\n",
    " 0.994057  3.25245  1.65416   0.251396  0.964566\n",
    " 1.03257   3.37871  1.71818   0.964566  0.140689\n",
    "\n",
    "julia> ForwardDiff.hessian(f, x) # H(f)(x) == J(∇f)(x), as expected \n",
    "5x5 Array{Float64,2}:\n",
    " 0.585111  3.48083  1.7706    0.994057  1.03257\n",
    " 3.48083   1.06079  5.79299   3.25245   3.37871\n",
    " 1.7706    5.79299  0.423981  1.65416   1.71818\n",
    " 0.994057  3.25245  1.65416   0.251396  0.964566\n",
    " 1.03257   3.37871  1.71818   0.964566  0.140689\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Julia?\n",
    "\n",
    "Unlike many other languages, **Julia's type-based operator overloading is fast and natural**, and one of the central design tenets of the langauge.\n",
    "\n",
    "This is because Julia is a dynamic, JIT-compiled language - **the compiled bytecode of a Julia function is tied directly to the types with which the function is called**, so the compiler can optimize every Julia method for the specific input type at runtime.\n",
    "\n",
    "For example:\n",
    "\n",
    "```julia\n",
    "julia> f(x) = sqrt(x)\n",
    "f (generic function with 1 method)\n",
    "\n",
    "julia> f(2) # method f(::Int64) is JIT-compiled and called\n",
    "1.4142135623730951\n",
    "\n",
    "julia> f(1+2im) # new method f(::Complex{Int64}) is JIT-compiled and called\n",
    "1.272019649514069 + 0.7861513777574233im\n",
    "\n",
    "julia> f(x::AbstractString) = \"√$x\" # if called on some sort of string, do this instead\n",
    "f (generic function with 2 methods)\n",
    "\n",
    "julia> f(\"2\") # new method f(::ASCIIString) is JIT-compiled and called\n",
    "\"√2\"\n",
    "\n",
    "julia> f(2) # call the version of f(::Int64) we already compiled\n",
    "1.4142135623730951\n",
    "```\n",
    "\n",
    "This strategy, referred to in Julia nomenclature as **multiple dispatch**, allows us to easily and efficiently overload core Julia methods (e.g. `sin`, `lgamma`, `log`, etc.)  to accumulate derivative information. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Types\n",
    "ForwardDiff.jl overloads mathematical methods on three different types:\n",
    "\n",
    "---\n",
    "\n",
    "The **`GradientNumber{N}`** type represents a **generalized dual number** with $N$ $\\epsilon$-components: \n",
    "$$g_N = a + \\sum_{i=1}^N b_i \\epsilon_i$$\n",
    "Overloaded functions on vectors of `GradientNumbers` can be structured to behave like: \n",
    "$$f(\\vec{x}_{g_N}) = f(\\vec{x}) + \\sum_{i=1}^N \\frac{\\delta f(\\vec{x})}{\\delta x_i} \\epsilon_i$$\n",
    "\n",
    "---\n",
    "\n",
    "The **`HessianNumber{N}`** type represents a **hyper-dual number** whose dimensions scale with $N$:\n",
    "$$h_N = a + \\sum_{i=1}^N b_i \\epsilon_i + \\sum_{i,j=1}^N c_{ij} \\epsilon_i \\epsilon_j$$\n",
    "Overloaded functions on vectors of `HessianNumbers` can be structured to behave like:\n",
    "$$f(\\vec{x}_{h_N}) = f(\\vec{x}_{g_N}) + \\sum_{i,j=1}^N \\frac{\\delta^2 f(\\vec{x})}{\\delta x_i \\delta x_j} \\epsilon_i \\epsilon_j$$\n",
    "\n",
    "---\n",
    "\n",
    "The **`TensorNumber{N}`** type represents a **$3^{\\text{rd}}$ order hyper-dual number** whose dimensions scale with $N$:\n",
    "$$t_N = a + \\sum_{i=1}^N b_i \\epsilon_i + \\sum_{i,j=1}^N c_{ij} \\epsilon_i \\epsilon_j + \\sum_{i,j,k=1}^N d_{ijk} \\epsilon_i \\epsilon_j \\epsilon_k$$\n",
    "Overloaded functions on vectors of `TensorNumbers` can be structured to behave like:\n",
    "$$f(\\vec{x}_{t_N}) = f(\\vec{x}_{h_N}) + \\sum_{i,j,k=1}^N \\frac{\\delta^3 f(\\vec{x})}{\\delta x_i \\delta x_j \\delta x_k} \\epsilon_i \\epsilon_j \\epsilon_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Vector-mode vs. Chunk-mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ForwardDiff.jl can be configured to use two different strategies for derivative evaluation.\n",
    "\n",
    "### vector-mode (default)\n",
    "Evaluate all partial derivatives of $f(\\vec{x})$ by attaching a unique $\\epsilon_i$ to each $x_i$. For example, taking the gradient of $f:\\mathbb{R}^N \\to \\mathbb{R}$ in vector-mode:\n",
    "\n",
    "$$\n",
    "\\vec{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_i \\\\\n",
    "\\vdots \\\\\n",
    "x_N\n",
    "\\end{bmatrix} \\to\n",
    "\\vec{x}_{g_N} =\n",
    "\\begin{bmatrix}\n",
    "x_1 + \\epsilon_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_i + \\epsilon_i \\\\\n",
    "\\vdots \\\\\n",
    "x_N + \\epsilon_N\n",
    "\\end{bmatrix} \\to\n",
    "f(\\vec{x}_{g_N}) =\n",
    "f(\\vec{x}) + \\sum_{i=1}^N \\frac{\\delta f(\\vec{x})}{\\delta x_i} \\epsilon_i\n",
    "$$\n",
    "\n",
    "Properties: \n",
    "\n",
    "- `N` == length($\\vec{x}$)\n",
    "- Partials Storage Type: Julia's heap-allocated `Vectors`$^*$.\n",
    "- Tradeoff: Only requires a single evaluation of $f$ to calculate any kind of derivative, but that single evaluation can require a lot of memory.\n",
    "\n",
    "$*_{\\text{Tuples are used in vector-mode when length(x) < 10, since there is little chance of \"clogging\" the stack at such low input dimensions.} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### chunk-mode\n",
    "Evaluate partial derivatives of $f(\\vec{x})$ in \"chunks\" of size $N$. For example, taking the gradient of $f:\\mathbb{R}^4 \\to \\mathbb{R}$ in chunk-mode with $N=2$:\n",
    "\n",
    "$$\n",
    "\\vec{x}_{g_{1,2}} = \n",
    "\\begin{bmatrix}\n",
    "x_1 + \\epsilon_1 \\\\\n",
    "x_2 + \\epsilon_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4\n",
    "\\end{bmatrix} \\to\n",
    "f(\\vec{x}_{g_{1,2}}) = f(\\vec{x}) + \\frac{\\delta f(\\vec{x})}{\\delta x_1} \\epsilon_1 + \\frac{\\delta f(\\vec{x})}{\\delta x_2} \\epsilon_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{x}_{g_{3,4}} = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 + \\epsilon_1\\\\\n",
    "x_4 + \\epsilon_2\n",
    "\\end{bmatrix} \\to\n",
    "f(\\vec{x}_{g_{3,4}}) = f(\\vec{x}) + \\frac{\\delta f(\\vec{x})}{\\delta x_3} \\epsilon_1 + \\frac{\\delta f(\\vec{x})}{\\delta x_4} \\epsilon_2\n",
    "$$\n",
    "\n",
    "\n",
    "- `N` <= length($\\vec{x}$)\n",
    "- Tradeoff: Multiple $f$ evaluations are required, but less memory is required per evaluation\n",
    "- Partials Storage Type: Julia's stack-allocated `Tuples` (fast member access with less GC overhead, but high dimensions can \"clog\" the stack).\n",
    "- Number of evaluations of $f$ to compute $(k = \\text{length}(x))$... \n",
    "    - $\\nabla f(\\vec{x})$: $\\frac{k}{N}$\n",
    "    - $J(f(\\vec{x}))$: $\\frac{k}{N}$\n",
    "    - $H(f(\\vec{x}))$: $\\frac{k}{N} (k + 1 - \\frac{1}{N}) \\to O(\\frac{k^2}{N})$\n",
    "    - $T(f(\\vec{x}))$: N/A (chunk-mode not supported for Tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "\n",
    "Every time shown below is the minimum of at least 5 trials.\n",
    "\n",
    "All ForwardDiff evaluations are performed in vector-mode unless it is specified otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ackley Function\n",
    "\n",
    "#### Definition\n",
    "\n",
    "$$f(\\vec{x}) = -a \\exp( -b \\sqrt{\\frac{1}{k} \\sum_{i=1}^k x^{2}_{i}}) - \\exp(\\frac{1}{k} \\sum_{i=1}^k \\cos(cx_{i})) + a + \\exp(1)$$\n",
    "\n",
    "#### Julia Code\n",
    "\n",
    "```julia\n",
    "function ackley(x)\n",
    "    a, b, c = 20.0, -0.2, 2.0*π\n",
    "    len_recip = inv(length(x))\n",
    "    sum_sqrs = zero(eltype(x))\n",
    "    sum_cos = sum_sqrs\n",
    "    for i in x\n",
    "        sum_cos += cos(c*i)\n",
    "        sum_sqrs += i*i\n",
    "    end\n",
    "    return (-a * exp(b * sqrt(len_recip*sum_sqrs)) -\n",
    "            exp(len_recip*sum_cos) + a + e)\n",
    "end\n",
    "```\n",
    "\n",
    "#### Python Code (with AlgoPy)\n",
    "\n",
    "```python\n",
    "def ackley(x):\n",
    "    a, b, c = 20.0, -0.2, 2.0*numpy.pi\n",
    "    len_recip = 1.0/len(x)\n",
    "    sum_sqrs, sum_cos = 0.0, 0.0\n",
    "    for i in x:\n",
    "        sum_cos += algopy.cos(c*i)\n",
    "        sum_sqrs += i*i\n",
    "    return (-a * algopy.exp(b*algopy.sqrt(len_recip*sum_sqrs)) -\n",
    "            algopy.exp(len_recip*sum_cos) + a + numpy.e)\n",
    "```\n",
    "\n",
    "#### Function evaluation time\n",
    "\n",
    "| length(x) | Python time (s) | Julia time (s) | Speed-Up vs. Python  |\n",
    "|-----------|-----------------|----------------|----------------------|\n",
    "| 16        | 5.60284e-5      | 2.74e-6        | 20.45                |\n",
    "| 1600      | 0.00446105      | 4.1718e-5      | 106.93               |\n",
    "| 16000     | 0.044596        | 0.000400089    | 111.47               |\n",
    "\n",
    "#### Gradient evaluation time\n",
    "\n",
    "| length(x) | AlgoPy time (s) | ForwardDiff time (s) | Speed-Up vs. AlgoPy |\n",
    "|-----------|-----------------|----------------------|---------------------|\n",
    "| 16        | 0.0019691       | 5.7223e-5            | 34.41               |\n",
    "| 1600      | 0.549358        | 0.0328962            | 16.70               |\n",
    "| 16000     | 89.5198         | 1.87736              | 47.68               |\n",
    "\n",
    "#### Function-to-gradient slowdown ratio (lower is better)\n",
    "\n",
    "$$\\text{per-partial slowdown ratio} = \\frac{\\text{gradient time}}{\\text{function time} \\cdot \\text{length}(\\vec{x})}$$\n",
    "\n",
    "| length(x) | AlgoPy Ratio  | ForwardDiff Ratio |\n",
    "|-----------|---------------|-------------------|\n",
    "| 16        | 2.20          | 1.31              |\n",
    "| 1600      | 0.08          | 0.49              |\n",
    "| 16000     | 0.13          | 0.29              |\n",
    "\n",
    "#### vector-mode vs. chunk-mode (ForwardDiff only)\n",
    "\n",
    "| length(x) | chunk size | chunk-mode time (s) | vector-mode time (s) | Speed-up vs. chunk-mode |\n",
    "|-----------|------------|---------------------|----------------------|-------------------------|\n",
    "| 16        | 1          | 5.6552e-5           | 5.7223e-5            | 0.99                    |\n",
    "| 16        | 2          | 4.9823e-5           | 5.7223e-5            | 0.87                    |\n",
    "| 16        | 4          | 4.6942e-5           | 5.7223e-5            | 0.82                    |\n",
    "| 16        | 8          | 4.5825e-5           | 5.7223e-5            | 0.80                    |\n",
    "| 16        | 16         | 4.4029e-5           | 5.7223e-5            | 0.78                    |\n",
    "| 1600      | 1          | 0.171796            | 0.0328962            | 5.22                    |\n",
    "| 1600      | 2          | 0.0966284           | 0.0328962            | 2.94                    |\n",
    "| 1600      | 4          | 0.0679323           | 0.0328962            | 2.07                    |\n",
    "| 1600      | 8          | 0.0484395           | 0.0328962            | 1.47                    |\n",
    "| 1600      | 16         | 0.0420936           | 0.0328962            | 1.28                    |\n",
    "| 16000     | 1          | 17.0998             | 1.87736              | 9.11                    |\n",
    "| 16000     | 2          | 9.59618             | 1.87736              | 5.11                    |\n",
    "| 16000     | 4          | 6.74976             | 1.87736              | 3.60                    |\n",
    "| 16000     | 8          | 4.80788             | 1.87736              | 2.56                    |\n",
    "| 16000     | 16         | 4.1689              | 1.87736              | 2.22                    |\n",
    "\n",
    "\n",
    "#### C++ Comparison\n",
    "\n",
    "| length(x) | function time (s)  | chunk size | chunk-mode time (s) | Speed-Up vs. ForwardDiff |\n",
    "|-----------|--------------------|------------|---------------------|--------------------------|\n",
    "| 16000     | 0.000397921        | 1          | 4.74544             | 3.60                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rosenbrock Function\n",
    "\n",
    "#### Definition\n",
    "\n",
    "$$f(\\vec{x}) = \\sum_{i=1}^{k-1} \\big[(b - x_i)^2 + a \\cdot (x_{i+1} - x_i^2)^2 \\big]$$\n",
    "#### Julia Code\n",
    "\n",
    "```julia\n",
    "sqr(i) = i*i\n",
    "\n",
    "function rosenbrock(x)\n",
    "    a, b = 100.0, 1.0\n",
    "    result = zero(eltype(x))\n",
    "    for i in 1:length(x)-1\n",
    "        result += sqr(b - x[i]) + a*sqr(x[i+1] - sqr(x[i]))\n",
    "    end\n",
    "    return result\n",
    "end\n",
    "```\n",
    "\n",
    "#### Python Code (with AlgoPy)\n",
    "\n",
    "```python\n",
    "def sqr(i):\n",
    "    return i*i\n",
    "\n",
    "def rosenbrock(x):\n",
    "    a, b = 100.0, 1.0\n",
    "    result = 0.0\n",
    "    for i in xrange(len(x)-1):\n",
    "        result += sqr(b - x[i]) + a*sqr(x[i+1] - sqr(x[i]))\n",
    "    return result\n",
    "```\n",
    "\n",
    "#### Function evaluation time\n",
    "\n",
    "| length(x) | Python time (s) | Julia time (s) | Speed-Up vs. Python  |\n",
    "|-----------|-----------------|----------------|----------------------|\n",
    "| 16        | 1.69277e-5      | 1.447e-6       | 11.70                |\n",
    "| 1600      | 0.00180316      | 5.428e-6       | 332.20               |\n",
    "| 16000     | 0.0162311       | 3.6963e-5      | 439.12               |\n",
    "\n",
    "#### Gradient evaluation time\n",
    "\n",
    "| length(x) | AlgoPy time (s) | ForwardDiff time (s) | Speed-Up vs. AlgoPy |\n",
    "|-----------|-----------------|----------------------|---------------------|\n",
    "| 16        | 0.00234795      | 5.8931e-5            | 39.84               |\n",
    "| 1600      | 0.605024        | 0.035377             | 17.10               |\n",
    "| 16000     | 98.9114         | 3.26842              | 30.26               |\n",
    "\n",
    "#### Function-to-gradient slowdown ratio (lower is better)\n",
    "\n",
    "$$\\text{per-partial slowdown ratio} = \\frac{\\text{gradient time}}{\\text{function time} \\cdot \\text{length}(\\vec{x})}$$\n",
    "\n",
    "| length(x) | AlgoPy Ratio  | ForwardDiff Ratio |\n",
    "|-----------|---------------|-------------------|\n",
    "| 16        | 8.70          | 2.55              |\n",
    "| 1600      | 0.21          | 4.07              |\n",
    "| 16000     | 0.38          | 5.53              |\n",
    "\n",
    "#### vector-mode vs. chunk-mode (ForwardDiff only)\n",
    "\n",
    "| length(x) | chunk size | chunk-mode time (s) | vector-mode time (s) | Speed-Up vs. chunk-mode |\n",
    "|-----------|------------|---------------------|----------------------|-------------------------|\n",
    "| 16        | 1          | 5.4143e-5           | 5.8931e-5            | 0.92                    |\n",
    "| 16        | 2          | 4.4274e-5           | 5.8931e-5            | 0.75                    |\n",
    "| 16        | 4          | 4.6301e-5           | 5.8931e-5            | 0.79                    |\n",
    "| 16        | 8          | 4.7975e-5           | 5.8931e-5            | 0.81                    |\n",
    "| 16        | 16         | 4.7291e-5           | 5.8931e-5            | 0.80                    |\n",
    "| 1600      | 1          | 0.13801             | 0.035377             | 3.90                    |\n",
    "| 1600      | 2          | 0.083086            | 0.035377             | 2.35                    |\n",
    "| 1600      | 4          | 0.0727295           | 0.035377             | 2.06                    |\n",
    "| 1600      | 8          | 0.0576288           | 0.035377             | 1.63                    |\n",
    "| 1600      | 16         | 0.0619054           | 0.035377             | 1.75                    |\n",
    "| 16000     | 1          | 13.7761             | 3.26842              | 4.21                    |\n",
    "| 16000     | 2          | 8.28223             | 3.26842              | 2.53                    |\n",
    "| 16000     | 4          | 7.26213             | 3.26842              | 2.22                    |\n",
    "| 16000     | 8          | 5.74794             | 3.26842              | 1.76                    |\n",
    "| 16000     | 16         | 6.20597             | 3.26842              | 1.90                    |\n",
    "\n",
    "#### C++ Comparison\n",
    "\n",
    "| length(x) | function time (s)  | chunk size | chunk-mode time (s) | Speed-Up vs. ForwardDiff |\n",
    "|-----------|--------------------|------------|---------------------|--------------------------|\n",
    "| 16000     | 3.50475e-5         | 1          | 0.751954            | 18.32                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0-dev",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
